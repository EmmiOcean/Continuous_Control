# Project: Solving a Continous Control Environment (Reacher) 
#### Udacity: DEEP-RL

This project deals with solving a continous control environment called Reacher with a modern Actor-Critic model approach. 

## Game environment

### Solving the Environment
The barrier for solving the second version of the environment is slightly different, to take into account the presence of many agents. In particular, your agents must get an average score of +30 (over 100 consecutive episodes, and over all agents). Specifically:
- After each episode, the rewards that each agent received are added to get a score for each agent. 
- Then average the scores by the number of agents (20).
- This has to be higher than +30.

The environment is considered solved, when the average (over 100 episodes) of those average scores is at least +30. 

## Getting Started
1. Check [Prerequisite](https://github.com/udacity/deep-reinforcement-learning/#dependencies), and follow the instructions.

2. Will just work on Linux system

3. Clone [?????????????????????this](h??????????????????????????) repository.

4. Place the file in `env/` directory, and unzip (or decompress) the file.

## Instructions
### Jupyter Notebook
To train the agent, start jupyter notebook, open `Navigation.ipynb` and execute! For more information, please check instructions inside the notebook.

### Command line 
To train the agent via command line type `python main.py --train`

To see the agent walking inside the enviroment use the command `python main.py --test`





# Banana_Joe_RI_Learning
