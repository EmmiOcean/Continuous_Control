{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control with Deep Deterministic Policy Gradient (DDPG)\n",
    "\n",
    "---\n",
    "\n",
    "This Projekt is the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "Here, an implementation of a Deep Deterministic Policy Gradient (DDPG) Algorithm is provided to solve the Unity Reacher environment.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from itertools import count\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from ddpg_agent import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate the Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='env_multiAgent/Reacher.x86_64')\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Agent\n",
    "agent = Agent(state_size = state_size, action_size= action_size, num_agents=num_agents, random_seed=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Agent with DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 1.23\n",
      "Episode 200\tAverage Score: 2.55\n",
      "Episode 300\tAverage Score: 3.88\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAE/lJREFUeJzt3X2QZXV95/H3h2GAWZ4myOgMzKwjyiz4wCL2smZFS3E1GRVIVhJI1hJSZikTDaa2UuHBFUErVrAS17WSCuLDZtQEMBhdMDsWKCRq7QL2xGEYHh0RFsHJNCLg+ACC3/3jnjm2bT/c6enTd7rv+1V1q88959z7+/44TX/m9zvnnpuqQpIkgH0GXYAkae9hKEiSWoaCJKllKEiSWoaCJKllKEiSWoaCJKllKEiSWoaCJKm176AL2F2HH354rV27dtBlSNKCsmnTpoerasVM+y24UFi7di2jo6ODLkOSFpQk9/ezn9NHkqSWoSBJahkKkqSWoSBJahkKkqSWoSBJahkKkqTWgvucwmxt3LiR7du3D7oMSZq1lStXsn79+k7bcKQgSWoNzUih63SVpMXAkYIkqWUoSJJaQzN9dONfX86O++8ddBmSNGvPfPZRvOrsczptw5GCJKk1NCOFrtNVkhYDRwqSpJahIElqdRoKSe5LcluSzUl+4evS0vOhJNuSbElyQpf1SJKmNx/nFF5VVQ9PsW09cHTz+PfAXzU/JUkDMOjpo9OAT1TPTcDyJKsGXJMkDa2uQ6GA65JsSjLZ5T9HAg+Me/7tZt3PSXJOktEko2NjYx2VKknqOhROqqoT6E0TvS3JK2bzJlV1eVWNVNXIihUr5rZCSVKr01CoqgebnzuAzwInTtjlQWDNuOerm3WSpAHoLBSSHJjk4F3LwGuBrRN2uwZ4c3MV0kuBx6rqO13VJEmaXpdXHz0L+GySXe38bVV9IclbAarqMuB/A68DtgE/BH6nw3okSTPoLBSq6l7g306y/rJxywW8rasaJEm7Z9CXpEqS9iKGgiSpZShIklqGgiSpZShIklqGgiSpZShIklqGgiSpZShIklqGgiSpZShIklqGgiSpZShIklqGgiSpZShIklqGgiSpZShIklqGgiSpZShIklqGgiSpZShIklqGgiSpZShIklqGgiSpZShIklqGgiSpZShIklr7DrqA+bL9fe/jiTvvGnQZkjRr+x97DCsvvLDTNhwpSJJaQzNS6DpdJWkxcKQgSWoZCpKklqEgSWoZCpKklqEgSWp1HgpJliT5epLPT7Lt7CRjSTY3j9/tuh5J0tTm45LUdwB3AodMsf2qqnr7PNQhSZpBpyOFJKuB1wMf7bIdSdLc6Hr66IPAHwM/nWafNybZkuTqJGsm2yHJOUlGk4yOjY11UqgkqcNQSPIGYEdVbZpmt2uBtVV1HHA9sGGynarq8qoaqaqRFStWdFCtJAm6HSm8DDg1yX3AlcDJST41foeq+m5VPdE8/Sjwkg7rkSTNoLNQqKoLqmp1Va0FzgRuqKo3jd8nyapxT0+ld0JakjQg835DvCTvAUar6hrg3CSnAk8BjwBnz3c9kqSfSVUNuobdMjIyUqOjo4MuQ5IWlCSbqmpkpv2G5tbZ99zzXr6/09kpSQvXwQcdy7p17+q0DW9zIUlqDc1Ioet0laTFYGhC4dJbLuWuR/yOZkkL1zGHHcN5J57XaRtOH0mSWkMzUug6XSVpMXCkIElqGQqSpNbQTB9dcu3t3PHQ44MuQ5Jm7flHHMK7T3lBp204UpAktYZmpNB1ukrSYuBIQZLUMhQkSS1DQZLUMhQkSS1DQZLUMhQkSS1DQZLUMhQkSS1DQZLUMhQkSS1DQZLUMhQkSa2huSHexo0b2b59+6DLkKRZW7lyJevXr++0DUcKkqRW3yOFJCcBR1fV/0yyAjioqr7VXWlzq+t0laTFoK+RQpJ3A+cBFzSrlgKf6qooSdJg9Dt99OvAqcAPAKrqIeDgroqSJA1Gv6HwZFUVUABJDuyuJEnSoPQbCp9O8mFgeZL/AnwR+Eh3ZUmSBqGvE81V9WdJXgM8Dvwb4KKqur7TyiRJ827GUEiyBPhiVb0KMAgkaRGbcfqoqp4Gfprk0HmoR5I0QP1+TmEncFuS62muQAKoqnM7qUqSNBD9hsLfN4/d1kw/jQIPVtUbJmzbH/gE8BLgu8AZVXXfbNqZyT33vJfv77yzi7eWpHlx8EHHsm7duzpto98TzRuS7Aesa1bdXVU/6bONdwB3AodMsu0twPeq6nlJzgQuBc7o830lSXOsr1BI8kpgA3AfEGBNkrOq6sszvG418HrgT4D/OskupwEXN8tXA3+RJM1nIuZU1+kqSYtBv9NHfw68tqruBkiyDriC3rTPdD4I/DFTf/r5SOABgKp6KsljwDOAh/usS5I0h/r98NrSXYEAUFX30Lv/0ZSSvAHYUVWb9qC+Xe91TpLRJKNjY2N7+naSpCn0GwqjST6a5JXN4yP0Th5P52XAqUnuA64ETk4y8SZ6DwJrAJLsCxxK74Tzz6mqy6tqpKpGVqxY0WfJkqTd1W8o/B5wB3Bu87ijWTelqrqgqlZX1VrgTOCGqnrThN2uAc5qlk9v9pnz8wmSpP70e05hX+B/VNUHoL3MdP/ZNJjkPcBoVV0DfAz4ZJJtwCP0wkOSNCD9hsKXgP9I70NsAMuA64D/0M+Lq+ofgX9sli8at/7HwG/0WcMeueTa27njocfnoylJ6sTzjziEd5/ygk7b6Hf66ICq2hUINMv/qpuSJEmD0u9I4QdJTqiqfwZIMgL8qLuy5l7X6SpJi0G/ofCHwN8leah5vgo/eSxJi86000dJ/l2SlVX1NeAY4CrgJ8AXgG/NQ32SpHk00zmFDwNPNsu/DFwI/CXwPeDyDuuSJA3ATNNHS6rqkWb5DODyqvoM8Jkkm7stTZI032YaKSxpPmkM8GrghnHb+j0fIUlaIGb6w34F8E9JHqZ3tdFXAJI8D3is49okSfNs2lCoqj9J8iV6VxtdN+4WFPsAf9B1cZKk+TXjFFBV3TTJunu6KUeSNEj9fqJZkjQEDAVJUstQkCS1DAVJUstQkCS1DAVJUstQkCS1huZWFe/6xrfZunNBfQWEJP2cFx60jPcevbrTNhwpSJJaQzNS6DpdJWkxcKQgSWoZCpKk1tBMHz167Td58qEfDLoMSZq1/Y44kOWnPLfTNoYmFB6851Hy6I8HXYYkzVrt/AnLO25jaEJh64+e5vGdTw26DEmatUP2f5oXdNzG0ITCEeuWs98DQ9NdSYvQ4WsO6ryNofkr+fLfXDfoEiRprzc0oXDpLZdy1yN3DboMSZq1Yw47hvNOPK/TNrwkVZLUGpqRQtfpKkmLgSMFSVLLUJAktQwFSVLLUJAktQwFSVKrs1BIckCSW5LcmuT2JJdMss/ZScaSbG4ev9tVPZKkmXV5SeoTwMlVtTPJUuCrSTZW1U0T9ruqqt7eYR2SpD51FgpVVcDO5unS5lFdtSdJ2nOdnlNIsiTJZmAHcH1V3TzJbm9MsiXJ1UnWdFmPJGl6nYZCVT1dVccDq4ETk7xwwi7XAmur6jjgemDDZO+T5Jwko0lGx8bGuixZkoZaerM889BQchHww6r6sym2LwEeqapDp3ufkZGRGh0d3e323/WNb7N15492+3WStLd44UHLeO/Rq2f12iSbqmpkpv26vPpoRZLlzfIy4DXAXRP2WTXu6anAnV3VI0maWZdXH60CNjQjgH2AT1fV55O8BxitqmuAc5OcCjwFPAKc3VUxs01XSRom8zZ9NFdmO30kScNs4NNHkqSFx1CQJLUMBUlSy1CQJLUMBUlSy1CQJLUMBUlSq8sPr+1Vbvzry9lx/72DLkOSZu2Zzz6KV519TqdtOFKQJLWGZqTQdbpK0mLgSEGS1DIUJEmtoZk+YuP5sP22QVchSbO38kWw/k87bcKRgiSpNTwjhY7TVZIWA0cKkqSWoSBJahkKkqSWoSBJahkKkqSWoSBJahkKkqSWoSBJag3Nh9e2v+99PHHnXYMuQ5Jmbf9jj2HlhRd22oYjBUlSa2hGCl2nqyQtBo4UJEktQ0GS1DIUJEktQ0GS1DIUJEktQ0GS1DIUJEktQ0GS1BqaD689eu03efKhHwy6DEmatf2OOJDlpzy30zY6GykkOSDJLUluTXJ7kksm2Wf/JFcl2Zbk5iRru6pHkjSzLkcKTwAnV9XOJEuBrybZWFU3jdvnLcD3qup5Sc4ELgXO6KKYrtNVkhaDzkYK1bOzebq0edSE3U4DNjTLVwOvTpKuapIkTa/TE81JliTZDOwArq+qmyfsciTwAEBVPQU8Bjxjkvc5J8loktGxsbEuS5akodZpKFTV01V1PLAaODHJC2f5PpdX1UhVjaxYsWJui5QkteblktSqehS4EfjVCZseBNYAJNkXOBT47nzUJEn6RV1efbQiyfJmeRnwGmDiV59dA5zVLJ8O3FBVE887SJLmSZdXH60CNiRZQi98Pl1Vn0/yHmC0qq4BPgZ8Msk24BHgzA7rkSTNoLNQqKotwIsnWX/RuOUfA7/RVQ2SpN3jbS4kSS1DQZLUMhQkSS1DQZLUGpq7pLLxfNh+26CrkKTZW/kiWP+nnTbhSEGS1BqekULH6SpJi4EjBUlSy1CQJLUMBUlSy1CQJLUMBUlSy1CQJLUMBUlSy1CQJLWy0L7oLMkYcP8sXno48PAclzMo9mXvZF/2Tval59lVNeOX3C+4UJitJKNVNTLoOuaCfdk72Ze9k33ZPU4fSZJahoIkqTVMoXD5oAuYQ/Zl72Rf9k72ZTcMzTkFSdLMhmmkIEmawVCEQpJfTXJ3km1Jzh90PbsryX1JbkuyOclos+6wJNcn+Ubz85cGXedkknw8yY4kW8etm7T29HyoOU5bkpwwuMp/0RR9uTjJg82x2ZzkdeO2XdD05e4kvzKYqn9RkjVJbkxyR5Lbk7yjWb/gjss0fVmIx+WAJLckubXpyyXN+uckubmp+aok+zXr92+eb2u2r52TQqpqUT+AJcA3gaOA/YBbgecPuq7d7MN9wOET1r0fOL9ZPh+4dNB1TlH7K4ATgK0z1Q68DtgIBHgpcPOg6++jLxcDfzTJvs9vftf2B57T/A4uGXQfmtpWASc0ywcD9zT1LrjjMk1fFuJxCXBQs7wUuLn57/1p4Mxm/WXA7zXLvw9c1iyfCVw1F3UMw0jhRGBbVd1bVU8CVwKnDbimuXAasKFZ3gD82gBrmVJVfRl4ZMLqqWo/DfhE9dwELE+yan4qndkUfZnKacCVVfVEVX0L2Ebvd3Hgquo7VfXPzfL3gTuBI1mAx2Wavkxlbz4uVVU7m6dLm0cBJwNXN+snHpddx+tq4NVJsqd1DEMoHAk8MO75t5n+l2ZvVMB1STYlOadZ96yq+k6zvB141mBKm5Wpal+ox+rtzbTKx8dN4y2IvjRTDi+m96/SBX1cJvQFFuBxSbIkyWZgB3A9vZHMo1X1VLPL+HrbvjTbHwOesac1DEMoLAYnVdUJwHrgbUleMX5j9caPC/IysoVce+OvgOcCxwPfAf58sOX0L8lBwGeAP6yqx8dvW2jHZZK+LMjjUlVPV9XxwGp6I5hj5ruGYQiFB4E1456vbtYtGFX1YPNzB/BZer8s/7JrCN/83DG4CnfbVLUvuGNVVf/S/I/8U+Aj/GwqYq/uS5Kl9P6I/k1V/X2zekEel8n6slCPyy5V9ShwI/DL9Kbr9m02ja+37Uuz/VDgu3va9jCEwteAo5sz+PvROyFzzYBr6luSA5McvGsZeC2wlV4fzmp2Owv4X4OpcFamqv0a4M3N1S4vBR4bN52xV5owt/7r9I4N9PpyZnOFyHOAo4Fb5ru+yTTzzh8D7qyqD4zbtOCOy1R9WaDHZUWS5c3yMuA19M6R3Aic3uw28bjsOl6nAzc0I7w9M+gz7vPxoHf1xD305ufeOeh6drP2o+hdLXErcPuu+unNHX4J+AbwReCwQdc6Rf1X0Bu+/4TefOhbpqqd3tUXf9kcp9uAkUHX30dfPtnUuqX5n3TVuP3f2fTlbmD9oOsfV9dJ9KaGtgCbm8frFuJxmaYvC/G4HAd8val5K3BRs/4oesG1Dfg7YP9m/QHN823N9qPmog4/0SxJag3D9JEkqU+GgiSpZShIklqGgiSpZShIklqGgoZGkqfH3TVzc2a4Y26StyZ58xy0e1+Sw2fxul9Jcklz99KNe1qH1I99Z95FWjR+VL1bCPSlqi7rspg+vJzeB5deDnx1wLVoSDhS0NBr/iX//vS+s+KWJM9r1l+c5I+a5XObe/ZvSXJls+6wJJ9r1t2U5Lhm/TOSXNfcE/+j9D78tautNzVtbE7y4SRLJqnnjOamaOcCH6R3m4bfSbJgPomvhctQ0DBZNmH66Ixx2x6rqhcBf0HvD/FE5wMvrqrjgLc26y4Bvt6suxD4RLP+3cBXq+oF9O5V9a8BkhwLnAG8rBmxPA3854kNVdVV9O72ubWp6bam7VP3pPNSP5w+0jCZbvroinE///sk27cAf5Pkc8DnmnUnAW8EqKobmhHCIfS+jOc/Nev/Icn3mv1fDbwE+Fpz2/tlTH0jw3XAvc3ygdX7rgCpc4aC1FNTLO/yenp/7E8B3pnkRbNoI8CGqrpg2p16X7l6OLBvkjuAVc100h9U1Vdm0a7UN6ePpJ4zxv38v+M3JNkHWFNVNwLn0btF8UHAV2imf5K8Eni4evfy/zLw28369cCuL3j5EnB6kmc22w5L8uyJhVTVCPAP9L5Z6/30boJ4vIGg+eBIQcNkWfMv7l2+UFW7Lkv9pSRbgCeA35rwuiXAp5IcSu9f+x+qqkeTXAx8vHndD/nZbYwvAa5Icjvwf4D/B1BVdyT5b/S+RW8fendbfRtw/yS1nkDvRPPvAx+YZLvUCe+SqqGX5D56t4N+eNC1SIPm9JEkqeVIQZLUcqQgSWoZCpKklqEgSWoZCpKklqEgSWoZCpKk1v8HCZXmFktUroIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train(n_episodes=300, print_every=100):\n",
    "    \n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    agent.reset()\n",
    "    all_scores_deque = deque(maxlen=print_every)\n",
    "    all_scores = []\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        \n",
    "        for _ in count():\n",
    "            actions = agent.act(env_info.vector_observations)\n",
    "            actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to the environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            scores += env_info.rewards                         # update the score (for each agent)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                break \n",
    "        all_scores_deque.append(scores)\n",
    "        all_scores.append(scores)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(all_scores_deque)), end=\"\")\n",
    "        torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "        torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(all_scores_deque)))\n",
    "            \n",
    "    return all_scores\n",
    "\n",
    "trainscores = train()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(trainscores)+1), trainscores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Watch a Smart Agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.actor_local.load_state_dict(torch.load('checkpoint_actor.pth'))\n",
    "agent.critic_local.load_state_dict(torch.load('checkpoint_critic.pth'))\n",
    "\n",
    "state = env.reset()(train_mode=False)[brain_name]\n",
    "for t in range(200):\n",
    "    action = agent.act(state, add_noise=False)\n",
    "    env.render()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "\n",
    "- Prioritized experience replay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
